{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import circuit_lib as cl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"apple_quality_balanced_dataset.csv\", header = 0)\n",
    "\n",
    "df.loc[df[\"Quality\"] == \"good\", \"Quality\"] = 1\n",
    "df.loc[df[\"Quality\"] == \"bad\", \"Quality\"] = 0\n",
    "df = df.drop(columns=[\"A_id\"])\n",
    "train = df.iloc[:850]\n",
    "test = df.iloc[850:]\n",
    "\n",
    "x_train = jnp.array(train.iloc[:,:7].to_numpy())\n",
    "y_train = jnp.array(train.iloc[:,7].to_numpy(dtype= np.float32))\n",
    "\n",
    "x_test = jnp.array(test.iloc[:,:7].to_numpy())\n",
    "y_test = jnp.array(test.iloc[:,7].to_numpy(dtype= np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reps = 3\n",
    "entanglement = cl.LINEAR\n",
    "\n",
    "dev = qml.device(\"lightning.gpu\", wires = 7)\n",
    "\n",
    "ZZFeatureMap = cl.create_ZZFeatureMap(dev)\n",
    "TwoLocal = cl.create_TwoLocal(dev, num_reps, entanglement)\n",
    "\n",
    "observable = qml.numpy.asarray([[0,0],\n",
    "                        [0,1]])\n",
    "\n",
    "@qml.qnode(dev, interface = \"jax\")\n",
    "def QuantumCircuit(params, x):\n",
    "    ZZFeatureMap(x)\n",
    "    TwoLocal(params)\n",
    "    return qml.expval(qml.Hermitian(observable, wires=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def abse(params, data, target):\n",
    "    return jnp.abs(target - QuantumCircuit(params, data))\n",
    "\n",
    "abse_map = jax.vmap(abse, (None, 0, 0))\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, data, target):\n",
    "    return jnp.mean(abse_map(params, data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.adam(1)\n",
    "max_steps = 100\n",
    "init_params = jnp.array(np.random.default_rng().random(size = (num_reps + 1, 7))*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def optimiser(params, data, training , print_training):\n",
    "    opt_state = opt.init(params)\n",
    "    args = (params, opt_state, jnp.asarray(data), jnp.asarray(training),print_training)\n",
    "    (params, opt_state, _, _, _) = jax.lax.fori_loop(0, max_steps+1, update_step_jit, args) \n",
    "    return params\n",
    "\n",
    "@jax.jit\n",
    "def update_step_jit(i,args):\n",
    "    # Unpacks the arguments\n",
    "    params, opt_state, data, targets, print_training = args\n",
    "    # Gets the loss and the gradients to be applied to the parameters, by passing in the loss function and the parameters, to see how the parameters perform \n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(params, data, targets)\n",
    "    #Prints the loss every 25 steps if print_training is enable\n",
    "    def print_fn():\n",
    "        jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val=loss_val)\n",
    "    jax.lax.cond((jnp.mod(i, 25) == 0 ) & print_training, print_fn, lambda: None)\n",
    "    #Applies the param updates and updates the optimiser states\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    #Returns the arguments to be resupplied in the next iteration\n",
    "    return (params, opt_state, data, targets, print_training)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melon/my-projects/NU25/PennylaneAnalysis/env/lib/python3.12/site-packages/pennylane/math/interface_utils.py:127: UserWarning: Contains tensors of types {'autograd', 'jax'}; dispatch will prioritize TensorFlow, PyTorch, and Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n",
      "/home/melon/my-projects/NU25/PennylaneAnalysis/env/lib/python3.12/site-packages/pennylane/devices/preprocess.py:286: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n",
      "\n",
      " Mark the parameters of the measured observables as non-trainable to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 0.5\n",
      "Step: 25  Loss: 0.5\n",
      "Step: 50  Loss: 0.5\n",
      "Step: 75  Loss: 0.5\n",
      "Step: 100  Loss: 0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "opt_params = optimiser(init_params, x_test, y_test, True)\n",
    "print(loss_fn(opt_params, x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
