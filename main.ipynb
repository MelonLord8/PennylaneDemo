{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import circuit_lib as cl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "print(jax.devices())\n",
    "jax.config.update('jax_enable_x64', True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"apple_quality_balanced_dataset.csv\", header = 0)\n",
    "\n",
    "df.loc[df[\"Quality\"] == \"good\", \"Quality\"] = 1\n",
    "df.loc[df[\"Quality\"] == \"bad\", \"Quality\"] = 0\n",
    "df = df.drop(columns=[\"A_id\"])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_x = scaler.fit_transform(df.iloc[:,:7])\n",
    "\n",
    "train = scaled_x[:850]\n",
    "test = scaled_x[850:]\n",
    "\n",
    "x_train = jnp.array(train)\n",
    "y_train = jnp.array(df.iloc[:850,7].to_numpy(dtype= np.float32))\n",
    "\n",
    "x_test = jnp.array(test)\n",
    "y_test = jnp.array(df.iloc[850:,7].to_numpy(dtype= np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0   0.177719  -2.681202  -0.847667  -0.817180   0.335193  -1.735526   \n",
      "1  -1.001741  -1.565734   1.461369   1.511607   0.897442  -3.641585   \n",
      "2   1.457602  -0.156201   0.901834   1.021681   1.112249   1.511980   \n",
      "3   0.156688  -4.783146  -0.484698  -0.754672  -2.574577  -0.356063   \n",
      "4   0.780096  -0.592755   2.002540  -3.219841  -0.585504   2.134278   \n",
      "\n",
      "   Feature_7  Quality  \n",
      "0   2.497458        0  \n",
      "1   1.453947        0  \n",
      "2   1.318246        1  \n",
      "3  -0.682549        0  \n",
      "4   1.464577        0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "n_samples = 1000  # Number of samples\n",
    "n_features = 7    # Number of features\n",
    "n_classes = 2     # Binary classification (0 and 1)\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=7,    # Total number of features\n",
    "    n_informative=7, # All features are informative\n",
    "    n_redundant=0,   # No redundant features\n",
    "    n_repeated=0,    # No repeated features\n",
    "    n_classes=2,     # Binary classification\n",
    "    random_state=42  # Reproducibility\n",
    ")\n",
    "\n",
    "# Convert to a DataFrame for better visualization and manipulation\n",
    "columns = [f\"Feature_{i+1}\" for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "df['Quality'] = y  # Add the target column\n",
    "\n",
    "# Save the dataset as a CSV file\n",
    "df.to_csv(\"binary_classification_dataset.csv\", index=False)\n",
    "\n",
    "# Print first few rows\n",
    "print(df.head())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df.loc[df[\"Quality\"] == \"good\", \"Quality\"] = 1\n",
    "df.loc[df[\"Quality\"] == \"bad\", \"Quality\"] = 0\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_x = scaler.fit_transform(df.iloc[:,:7])\n",
    "\n",
    "x_train = scaled_x[:40]\n",
    "x_test = scaled_x[850:]\n",
    "\n",
    "y_train = df.iloc[:40,7].to_numpy(dtype= np.float32)\n",
    "y_test = df.iloc[850:,7].to_numpy(dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires = 7)\n",
    "\n",
    "ZZFeatureMap = cl.create_ZZFeatureMap(dev)\n",
    "\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def EmbeddingCircuit(x):\n",
    "    ZZFeatureMap(x)\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def embeddor(x):\n",
    "    return EmbeddingCircuit(x)\n",
    "\n",
    "embedding_map = jax.vmap(embeddor,[0])\n",
    "\n",
    "x_train_embedding = embedding_map(x_train)\n",
    "x_test_embedding = embedding_map(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reps = 12\n",
    "entanglement = cl.LINEAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires = 7)\n",
    "ZZFeatureMap = cl.create_ZZFeatureMap(dev)\n",
    "TwoLocal = cl.create_TwoLocal(dev,num_reps,entanglement)\n",
    "@qml.qnode(dev, interface = \"jax\")\n",
    "def QuantumCircuit(params, x):\n",
    "    ZZFeatureMap(x)\n",
    "    TwoLocal(params)\n",
    "    return qml.probs(wires=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def BCE(params, data, target):\n",
    "    sigmoid = 1/(1+ jnp.exp(-1*QuantumCircuit(params, data)[0]))\n",
    "    sigmoid = jnp.clip(sigmoid, 1e-7, 1 - 1e-7)\n",
    "    return  (target * jnp.log(sigmoid) + (1 - target) * jnp.log(1 - sigmoid))\n",
    "\n",
    "BCE_map = jax.vmap(BCE, (None, 0, 0))\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, data, target):\n",
    "    return jnp.mean(BCE_map(params, data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_schedule = optax.schedules.join_schedules(schedules = [optax.constant_schedule((0.5)**i) for i in range(6)],\n",
    "                                                                     boundaries = [100, 250, 500, 700, 900])\n",
    "opt = optax.lbfgs(1e-8)\n",
    "max_steps = 200\n",
    "@jax.jit\n",
    "def f(params):\n",
    "    return loss_fn(params, x_train, y_train)\n",
    "value_and_grad = optax.value_and_grad_from_state(f)\n",
    "@jax.jit\n",
    "def optimiser(params, data, targets, print_training):\n",
    "    param_history = jnp.zeros(((max_steps + 1),) + params.shape)  \n",
    "    grad_history = jnp.zeros(((max_steps + 1),) + params.shape)  \n",
    "    opt_state = opt.init(params)\n",
    "    #Packages the arguments to be sent to the function update_step_jit \n",
    "    args = (params, opt_state, jnp.asarray(data), targets, print_training, param_history, grad_history)\n",
    "    #Loops max_steps number of times\n",
    "    (params, opt_state, _, _, _, param_history, grad_history) = jax.lax.fori_loop(0, max_steps+1, update_step_jit, args) \n",
    "    return params, param_history, grad_history\n",
    "\n",
    "@jax.jit\n",
    "def update_step_jit(i,args):\n",
    "    # Unpacks the arguments\n",
    "    params, opt_state, data, targets, print_training, param_history, grad_history = args\n",
    "    param_history = param_history.at[i].set(params)\n",
    "    # Gets the loss and the gradients to be applied to the parameters, by passing in the loss function and the parameters, to see how the parameters perform \n",
    "    loss_val, grads = value_and_grad(params, state = opt_state)\n",
    "    grad_history = grad_history.at[i].set(grads)\n",
    "    #Prints the loss every 25 steps if print_training is enable\n",
    "    def print_fn():\n",
    "        jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val= loss_val)\n",
    "    jax.lax.cond((jnp.mod(i, 50) == 0 ) & print_training, print_fn, lambda: None)\n",
    "    #Applies the param updates and updates the optimiser states\n",
    "    updates, opt_state = opt.update(\n",
    "            grads, opt_state, params, value=loss_val, grad=grads, value_fn=f\n",
    "        )\n",
    "    params =  optax.apply_updates(params, updates)\n",
    "    #Returns the arguments to be resupplied in the next iteration\n",
    "    return (params, opt_state, data, targets, print_training, param_history, grad_history)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: -0.7990769841801065\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng()\u001b[38;5;241m.\u001b[39mrandom(size \u001b[38;5;241m=\u001b[39m (num_reps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m)))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpi \u001b[38;5;241m-\u001b[39m pi\n\u001b[0;32m      3\u001b[0m params, params_hist, grad_hist \u001b[38;5;241m=\u001b[39m optimiser(params, x_train, y_train, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_fn(params, \u001b[43mx_test_embedding\u001b[49m, y_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test_embedding' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50  Loss: -0.7990769841801065\n",
      "Step: 100  Loss: -0.7990769841801065\n",
      "Step: 150  Loss: -0.7990769841801065\n",
      "Step: 200  Loss: -0.7990769841801065\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "params = jnp.array(np.random.default_rng().random(size = (num_reps + 1, 7)))*2*pi - pi\n",
    "params, params_hist, grad_hist = optimiser(params, x_train, y_train, True)\n",
    "print(loss_fn(params, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.16809215 -0.46130867  2.24321916  1.8193395  -0.41483542 -1.4582453\n",
      "  -0.84811708]\n",
      " [ 0.82518368 -1.00287381 -1.61947866 -0.71348055  2.20479855 -1.64447686\n",
      "   1.96090013]\n",
      " [-1.28673911  1.49313392 -0.0878167  -0.02091742  1.92579819 -0.87373926\n",
      "  -2.97448706]\n",
      " [-2.88824842  0.82052263  1.35324466  2.05261903  0.84653078 -1.12220721\n",
      "  -0.3323143 ]]\n",
      "[[ 1.31798885 -0.46130867  2.24321916  1.8193395  -0.41483542 -1.4582453\n",
      "  -0.84811708]\n",
      " [ 1.31980402 -1.00287381 -1.61947866 -0.71348055  2.20479855 -1.64447686\n",
      "   1.96090013]\n",
      " [-1.04911699  1.49313392 -0.0878167  -0.02091742  1.92579819 -0.87373926\n",
      "  -2.97448706]\n",
      " [-2.43080033  0.82052263  1.35324466  2.05261903  0.84653078 -1.12220721\n",
      "  -0.3323143 ]]\n",
      "(4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(params_hist[0])\n",
    "print(params_hist[500])\n",
    "print(params_hist[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.00162597e-04  1.73472348e-18  3.46944695e-18  0.00000000e+00\n",
      "   0.00000000e+00 -6.93889390e-18 -3.46944695e-18]\n",
      " [-1.34199328e-03  3.46944695e-18  0.00000000e+00  0.00000000e+00\n",
      "  -3.46944695e-18  0.00000000e+00  3.46944695e-18]\n",
      " [-1.21186954e-03  0.00000000e+00  4.33680869e-19 -2.16840434e-19\n",
      "  -3.46944695e-18  3.46944695e-18 -8.67361738e-19]\n",
      " [-3.98594499e-03 -6.93889390e-18 -3.46944695e-18  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "[[ 1.55322990e-04  1.73472348e-18  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00 -3.46944695e-18  0.00000000e+00]\n",
      " [-1.49050115e-03 -3.46944695e-18  3.46944695e-18  3.46944695e-18\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.88159813e-05 -6.93889390e-18  0.00000000e+00  1.08420217e-18\n",
      "  -3.46944695e-18  0.00000000e+00  0.00000000e+00]\n",
      " [-2.97333308e-03  0.00000000e+00  0.00000000e+00  6.93889390e-18\n",
      "   3.46944695e-18  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(grad_hist[0])\n",
    "print(grad_hist[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([QuantumCircuit(params,x_test[i]) for i in range(40)])\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.054754496248425655\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
